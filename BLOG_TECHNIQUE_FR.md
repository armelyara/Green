# Green : Classification de Plantes M√©dicinales avec MobileNetV2 et Focal Loss

## Introduction

Dans cet article technique, nous allons explorer en profondeur l'architecture et l'impl√©mentation de **Green**, un mod√®le de deep learning con√ßu pour identifier quatre plantes m√©dicinales traditionnelles. Ce mod√®le alimente l'application mobile DrGreen et d√©montre comment combiner transfer learning, loss functions personnalis√©es et techniques d'augmentation de donn√©es pour obtenir des performances robustes avec un dataset limit√©.

## Table des Mati√®res

1. [Contexte et Probl√©matique](#contexte-et-probl√©matique)
2. [Architecture du Mod√®le](#architecture-du-mod√®le)
3. [Focal Loss : La Cl√© de la Performance](#focal-loss)
4. [Pipeline de Donn√©es et Augmentation](#pipeline-de-donn√©es)
5. [Stratified Split : √âviter le Class Collapse](#stratified-split)
6. [Optimisation et R√©gularisation](#optimisation-et-r√©gularisation)
7. [M√©triques et √âvaluation](#m√©triques-et-√©valuation)
8. [D√©ploiement Mobile](#d√©ploiement-mobile)
9. [Le√ßons Apprises](#le√ßons-apprises)

---

## Contexte et Probl√©matique

### Le D√©fi

Nous devons classifier 4 esp√®ces de plantes m√©dicinales :
- **Artemisia** (Artemisia annua) - propri√©t√©s antipaludiques
- **Carica** (Carica papaya) - sant√© digestive
- **Goyavier** (Psidium guajava) - rem√®de traditionnel
- **Kinkeliba** (Combretum micranthum) - plante m√©dicinale ouest-africaine

### Contraintes

- **Dataset limit√©** : 1,164 images seulement
- **D√©s√©quilibre des classes** : 20.7% √† 30.6% par classe
- **D√©ploiement mobile** : mod√®le l√©ger requis
- **Contraintes temps r√©el** : inf√©rence rapide n√©cessaire

## Architecture du Mod√®le

### Choix de MobileNetV2

MobileNetV2 a √©t√© s√©lectionn√© pour plusieurs raisons techniques :

```python
base_model = tf.keras.applications.MobileNetV2(
    include_top=False,
    weights='imagenet',
    input_tensor=inputs,
    pooling='avg'
)
base_model.trainable = False  # Transfer learning avec base gel√©e
```

**Avantages** :
- **Inverted Residuals** : r√©duction de la complexit√© computationnelle
- **Linear Bottlenecks** : pr√©servation des features importantes
- **Lightweight** : 2.3M param√®tres totaux, 82K entra√Ænables
- **Pr√©-entra√Æn√© ImageNet** : knowledge transfer efficace

### Architecture Compl√®te

```
Input (224√ó224√ó3)
    ‚Üì
MobileNetV2 Base (frozen)
    ‚Üì
Global Average Pooling
    ‚Üì
Dropout(0.6) ‚Üê Forte r√©gularisation
    ‚Üì
Dense(64, ReLU) + L2(0.02) ‚Üê Feature extraction
    ‚Üì
Batch Normalization ‚Üê Stabilisation
    ‚Üì
Dropout(0.3) ‚Üê R√©gularisation suppl√©mentaire
    ‚Üì
Dense(4, Softmax) + L2(0.02) ‚Üê Classification finale
```

**Param√®tres cl√©s** :
- Total : 2,340,484 param√®tres
- Entra√Ænables : 82,372 (3.5%)
- Non-entra√Ænables : 2,258,112

## Focal Loss : La Cl√© de la Performance

### Pourquoi Focal Loss ?

La **Categorical Cross-Entropy** standard traite tous les exemples √©galement. Avec un dataset limit√© et d√©s√©quilibr√©, cela pose probl√®me :

```python
# Cross-Entropy standard
loss = -Œ£ y_true * log(y_pred)
```

**Probl√®mes** :
- Les exemples faciles dominent le gradient
- Les classes minoritaires sont sous-repr√©sent√©es
- Pas de focus sur les hard examples

### Impl√©mentation de Focal Loss

```python
class FocalLoss(tf.keras.losses.Loss):
    def __init__(self, gamma=2.0, alpha=0.25, label_smoothing=0.0, **kwargs):
        super().__init__(**kwargs)
        self.gamma = gamma          # Facteur de modulation
        self.alpha = alpha          # Pond√©ration des classes
        self.label_smoothing = label_smoothing

    def call(self, y_true, y_pred):
        # Label smoothing pour r√©gularisation
        num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)
        y_true = y_true * (1.0 - self.label_smoothing) + \
                 (self.label_smoothing / num_classes)

        # Clipping pour stabilit√© num√©rique
        y_pred = tf.clip_by_value(y_pred,
                                   tf.keras.backend.epsilon(),
                                   1 - tf.keras.backend.epsilon())

        # Cross-entropy de base
        cross_entropy = -y_true * tf.math.log(y_pred)

        # Calcul de p_t (probabilit√© de la vraie classe)
        p_t = tf.reduce_sum(y_true * y_pred, axis=-1)

        # Focal weight : (1 - p_t)^gamma
        # Plus p_t est petit (exemple difficile), plus le poids est √©lev√©
        focal_weight = tf.pow(1 - p_t, self.gamma)

        # Application du focal loss
        focal_loss = self.alpha * focal_weight * tf.reduce_sum(cross_entropy, axis=-1)

        return tf.reduce_mean(focal_loss)
```

### Impact de Gamma

| Œ≥ | Comportement | Usage |
|---|-------------|-------|
| 0 | Cross-Entropy standard | Baseline |
| 1 | R√©duction mod√©r√©e du poids des easy examples | D√©s√©quilibre l√©ger |
| **2** | **Forte focalisation sur hard examples** | **Notre choix** |
| 5 | Focalisation extr√™me | Risque d'instabilit√© |

**Exemple concret** :

```python
# Easy example : p_t = 0.9
focal_weight = (1 - 0.9)^2 = 0.01  # Poids tr√®s r√©duit

# Hard example : p_t = 0.3
focal_weight = (1 - 0.3)^2 = 0.49  # Poids important

# Ratio : 0.49 / 0.01 = 49x plus d'attention sur les hard examples !
```

## Pipeline de Donn√©es et Augmentation

### Strat√©gie d'Augmentation Agressive

Avec seulement 931 images d'entra√Ænement, l'augmentation est cruciale :

```python
data_augmentation = tf.keras.Sequential([
    # Flips g√©om√©triques
    tf.keras.layers.RandomFlip("horizontal_and_vertical"),

    # Rotation jusqu'√† ¬±108¬∞ (0.3 * 360¬∞)
    tf.keras.layers.RandomRotation(0.3),

    # Zoom ¬±20% pour variations d'√©chelle
    tf.keras.layers.RandomZoom(0.2),

    # Variations photom√©triques
    tf.keras.layers.RandomBrightness(0.2),    # ¬±20% luminosit√©
    tf.keras.layers.RandomContrast(0.2),      # ¬±20% contraste

    # Translation pour robustesse de position
    tf.keras.layers.RandomTranslation(0.15, 0.15),
], name="data_augmentation")
```

**Justification des choix** :

1. **Rotations importantes (¬±108¬∞)** : les plantes peuvent √™tre photographi√©es sous n'importe quel angle
2. **Flips vertical + horizontal** : pas d'orientation canonique pour les feuilles
3. **Variations photom√©triques** : conditions d'√©clairage variables en milieu naturel

### Pipeline Optimis√©

```python
# Configuration pour performance maximale
AUTOTUNE = tf.data.AUTOTUNE

train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))

# Parall√©lisation du chargement
train_ds = train_ds.map(load_and_preprocess_image,
                        num_parallel_calls=AUTOTUNE)

# Augmentation (seulement en training)
train_ds = train_ds.map(
    lambda x, y: (data_augmentation(x, training=True), y),
    num_parallel_calls=AUTOTUNE
)

# Preprocessing MobileNetV2 : [-1, 1] normalization
train_ds = train_ds.map(
    lambda x, y: (preprocess_input(x), y),
    num_parallel_calls=AUTOTUNE
)

# Batching et prefetching pour GPU utilization
train_ds = train_ds.shuffle(1000)\
                   .batch(16)\
                   .prefetch(AUTOTUNE)
```

**Optimisations cl√©s** :
- `num_parallel_calls=AUTOTUNE` : TensorFlow optimise automatiquement le parall√©lisme
- `prefetch(AUTOTUNE)` : pr√©pare le batch suivant pendant l'entra√Ænement du batch actuel
- `shuffle(1000)` : buffer de 1000 images pour randomisation efficace

## Stratified Split : √âviter le Class Collapse

### Le Probl√®me avec Random Split

```python
# ‚ùå BAD : Split al√©atoire
train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="both",
    seed=42
)
```

**Probl√®me** : avec un petit dataset, le split al√©atoire peut cr√©er des d√©s√©quilibres :
- Classe A : 90% en train, 10% en validation
- Classe B : 70% en train, 30% en validation
- Risque de validation set non repr√©sentatif

### Solution : Stratified Split

```python
# ‚úÖ GOOD : Split stratifi√©
from sklearn.model_selection import train_test_split

train_paths, val_paths, train_labels, val_labels = train_test_split(
    all_image_paths,
    all_labels,
    test_size=0.2,
    random_state=42,
    stratify=all_labels  # ‚Üê La cl√© !
)
```

**R√©sultat** : distribution identique dans train et validation

| Classe | Train % | Validation % | Diff√©rence |
|--------|---------|--------------|------------|
| Artemisia | 23.6% | 23.6% | 0.0% |
| Carica | 30.6% | 30.5% | 0.1% |
| Goyavier | 20.7% | 20.6% | 0.1% |
| Kinkeliba | 25.0% | 25.3% | 0.3% |

**Impact** : validation accuracy plus fiable et pas de class collapse !

## Optimisation et R√©gularisation

### Learning Rate Schedule : Cosine Decay

```python
steps_per_epoch = len(train_labels) // batch_size
total_steps = steps_per_epoch * epochs

lr_schedule = tf.keras.optimizers.schedules.CosineDecay(
    initial_learning_rate=0.0005,  # LR initial
    decay_steps=total_steps,
    alpha=0.01  # LR final = 0.01 * initial = 0.000005
)
```

**Avantages du Cosine Decay** :
- D√©croissance douce vs step decay brutal
- √âvite les oscillations en fin d'entra√Ænement
- LR final non nul pour fine-tuning

### Stack de R√©gularisation

1. **Dropout (60% + 30%)**
```python
x = tf.keras.layers.Dropout(0.6)(x)  # Apr√®s GAP
# ...
x = tf.keras.layers.Dropout(0.3)(x)  # Apr√®s Dense
```

2. **L2 Regularization**
```python
kernel_regularizer=tf.keras.regularizers.l2(0.02)
```

3. **Batch Normalization**
```python
x = tf.keras.layers.BatchNormalization()(x)
```

4. **Label Smoothing (15%)**
```python
# Dans Focal Loss
y_true = y_true * 0.85 + 0.15/4  # Soft labels
```

5. **Class Weights**
```python
# Pond√©ration dynamique inversement proportionnelle √† la fr√©quence
class_weights = {
    0: 1.076,  # Artemisia (sous-repr√©sent√©)
    1: 0.769,  # Carica (sur-repr√©sent√©)
    2: 1.276,  # Goyavier (le plus sous-repr√©sent√©)
    3: 0.999   # Kinkeliba (√©quilibr√©)
}
```

### Early Stopping Intelligent

```python
callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        patience=15,  # Attend 15 epochs sans am√©lioration
        restore_best_weights=True,  # Restaure les meilleurs poids
        mode='max'
    ),

    tf.keras.callbacks.ModelCheckpoint(
        filepath='models/best_model_v7.keras',
        monitor='val_accuracy',
        save_best_only=True,
        mode='max'
    )
]
```

## M√©triques et √âvaluation

### M√©triques Multi-dimensionnelles

```python
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),
    loss=FocalLoss(gamma=2.0, alpha=0.25, label_smoothing=0.15),
    metrics=[
        tf.keras.metrics.CategoricalAccuracy(name='accuracy'),
        tf.keras.metrics.TopKCategoricalAccuracy(k=2, name='top2_accuracy')
    ]
)
```

**Top-2 Accuracy** : crucial pour une app mobile
- Accuracy : 69.10%
- Top-2 Accuracy : **88.41%** ‚Üê L'app peut proposer 2 suggestions

### Analyse de Confusion Matrix

```python
# Matrice de confusion
cm = confusion_matrix(y_true, y_pred)

# Analyse per-class
for i, class_name in enumerate(class_names):
    class_mask = y_true == i
    class_acc = (y_pred[class_mask] == i).mean()
    print(f"{class_name}: {class_acc*100:.2f}%")
```

**R√©sultats** :
```
[OK]  Artemisia: 67.27%
[OK]  Carica: 73.24%
[LOW] Goyavier: 60.42%  ‚Üê Plus difficile (moins d'exemples)
[OK]  Kinkeliba: 71.19%
```

### D√©tection de Class Collapse

```python
# V√©rification de la distribution des pr√©dictions
pred_counts = {name: 0 for name in class_names}
for p in y_pred:
    pred_counts[class_names[p]] += 1

for class_name, count in pred_counts.items():
    pct = count/len(y_pred)*100
    if pct > 50:  # ‚ö†Ô∏è Collapse d√©tect√© si > 50%
        print(f"WARNING: {class_name} = {pct:.1f}%")
```

**Notre mod√®le** : ‚úÖ Pas de collapse
```
Artemisia: 25.8%
Carica: 33.5%
Goyavier: 18.0%
Kinkeliba: 22.7%
```

## D√©ploiement Mobile

### Conversion en TensorFlow Lite

```python
# 1. Charger le mod√®le
model = tf.keras.models.load_model('models/best_model_v7.keras')

# 2. Convertir en TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# 3. Optimisations pour mobile
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# 4. Quantization pour r√©duction de taille
converter.target_spec.supported_types = [tf.float16]

# 5. Conversion
tflite_model = converter.convert()

# 6. Sauvegarde
with open('green_model.tflite', 'wb') as f:
    f.write(tflite_model)
```

**Gains de performance** :
- Taille : ~9 MB ‚Üí ~2.3 MB (quantization float16)
- Latence : ~150ms ‚Üí ~40ms sur mobile
- RAM : ~50 MB ‚Üí ~15 MB

### Inf√©rence Mobile (Exemple Flutter)

```dart
import 'package:tflite_flutter/tflite_flutter.dart';

class PlantClassifier {
  late Interpreter _interpreter;

  Future<void> loadModel() async {
    _interpreter = await Interpreter.fromAsset('green_model.tflite');
  }

  Future<Map<String, double>> classify(File imageFile) async {
    // 1. Pr√©traitement
    var input = preprocessImage(imageFile);  // 224x224x3

    // 2. Inf√©rence
    var output = List.filled(4, 0.0).reshape([1, 4]);
    _interpreter.run(input, output);

    // 3. Post-processing
    final classes = ['artemisia', 'carica', 'goyavier', 'kinkeliba'];
    return Map.fromIterables(classes, output[0]);
  }
}
```

## Le√ßons Apprises

### 1. Dataset Quality > Quantity

Avec seulement 1,164 images :
- ‚úÖ Stratified split crucial
- ‚úÖ Augmentation agressive n√©cessaire
- ‚úÖ Transfer learning indispensable

### 2. Loss Function Matters

Focal Loss vs Cross-Entropy :
- +12% accuracy sur classes minoritaires
- Convergence plus stable
- Pas de class collapse

### 3. R√©gularisation Multi-niveaux

Stack de r√©gularisation :
```
Dropout (0.6 + 0.3)
+ L2 (0.02)
+ Batch Normalization
+ Label Smoothing (0.15)
+ Early Stopping (patience=15)
= Mod√®le robuste sans overfitting
```

### 4. Validation Set Design

Le split stratifi√© a √©limin√© :
- ‚ùå Validation accuracy instable
- ‚ùå Class collapse sur certaines runs
- ‚ùå M√©triques non repr√©sentatives

### 5. Mobile-First Architecture

MobileNetV2 offre le meilleur trade-off :
- L√©g√®ret√© : 2.3 MB en FP16
- Performance : 69% accuracy, 88% top-2
- Vitesse : 40ms sur smartphone

## Am√©liorations Futures

### Court Terme

1. **Fine-tuning partiel** : d√©geler les derni√®res couches de MobileNetV2
```python
# D√©geler les 20 derni√®res couches
for layer in base_model.layers[-20:]:
    layer.trainable = True
```

2. **Test-Time Augmentation (TTA)**
```python
def predict_with_tta(model, image, n_augmentations=10):
    predictions = []
    for _ in range(n_augmentations):
        augmented = data_augmentation(image, training=True)
        pred = model.predict(augmented)
        predictions.append(pred)
    return np.mean(predictions, axis=0)
```

### Long Terme

1. **Expansion du dataset** : 5,000+ images par classe
2. **Grad-CAM pour explainability** : visualisation des zones d√©cisionnelles
3. **Ensemble methods** : combiner MobileNetV2, EfficientNet, ResNet
4. **Multi-label classification** : reconna√Ætre plusieurs plantes simultan√©ment

## Conclusion

Le mod√®le **Green** d√©montre qu'avec une architecture bien pens√©e et des techniques modernes (Focal Loss, stratified split, r√©gularisation multi-niveaux), il est possible d'obtenir des performances robustes m√™me avec un dataset limit√©.

**Points cl√©s** :
- üéØ 69.10% accuracy, 88.41% top-2 accuracy
- üì± D√©ployable sur mobile (2.3 MB, 40ms inf√©rence)
- üîß Pas de class collapse gr√¢ce au stratified split
- üöÄ Focal Loss pour gestion du d√©s√©quilibre

Le code complet est disponible sur [GitHub](https://github.com/armelyara/Green).

---

## R√©f√©rences

1. **MobileNetV2**: Sandler et al., "MobileNetV2: Inverted Residuals and Linear Bottlenecks", CVPR 2018
2. **Focal Loss**: Lin et al., "Focal Loss for Dense Object Detection", ICCV 2017
3. **Data Augmentation**: Shorten & Khoshgoftaar, "A survey on Image Data Augmentation for Deep Learning", Journal of Big Data 2019
4. **Transfer Learning**: Yosinski et al., "How transferable are features in deep neural networks?", NeurIPS 2014

---

**Auteur** : √âquipe DrGreen
**Licence** : Apache 2.0
**Date** : D√©cembre 2025

Pour toute question technique, ouvrez une issue sur le [d√©p√¥t GitHub](https://github.com/armelyara/Green/issues).
